I""p<p>실습</p>

<p>Client머신에서 실습.
HDD 1GB(SCSI) 추가한 뒤 파티션을 100, 200, 300, 400Mbyte로 나눈다. 파일시스템을 만든 뒤 마운트를 진행한다.</p>

<p>nfs server 입장에서는
/etc/exports 설정을 통해 300Mbyte의 용량을 지니고 있는 마운트포인트 디렉토리를 nfs 서비스로 제공한다. (네트워크 대역방식으로. 옵션은 자유롭게)
/etc/fstab 설정을 통해 auto mount를 진행한다.</p>

<p>→ systemctl restart nfs.service 필요할 것</p>

<p>nfs client입장에서는
mount -t nfs 가 아닌 autofs로 서버의 nfs서비스를 이용한다.
nfs client가 자신의 마운트포인트에서 파일이나 디렉토리 생성 후 nfs server에서도 공유되는지 확인(재부팅 후에도 확인)</p>

<p>→ autofs패키지가 없다면 설치해야하며 패키지 설치 후 설정파일을 건드려야 한다.(/etc/autofs.conf, /etc/auto.master, /etc/auto.misc)
이후에는 systemctl restart autofs.service로 서비스를 시작시켜줘야 /misc 디렉토리가 생성 될 것.</p>

<p>⇒ 재부팅 후에는 nfs 서비스도 autofs서비스도 꺼져있을 것이니 둘 다 재실행 시켜줘야 한다. 이 때 nfs는 firewall-cmd –permanent –add-service= 으로 추가가 가능한데 autofs는 firewall-cmd –get-services했을 때 그 이름이 보이지가 않는다.
또한 현재 하나의 머신으로 서버와 클라이언트를 모두 수행했으므로 방화벽설정은 필요하지 않았다.</p>

<p>firewall-cmd로 서비스를 키고 끄는 것은 단순히 외부에서 내부로 접근을 하려 할 때 해당 서비스(포트)를 열어주는 것일뿐 재부팅뒤에서 서비스 자체가 자동으로 켜지도록 만드는 것은 아니다. 방화벽설정에서 nfs서비스는 외부에서 접근을 하려 하는 부분이니 해당 서비스항목이 있는 것이고.. autofs를 통해 외부에서 접근하려는 것은 똑같이 nfs를 이용하려는 것이므로 autofs라는 해당 항목에 대해 별개로 포트개방용 서비스가 존재하지 않는 것으로 보인다.</p>

<p>풀이</p>

<p>하드디스크를 추가한다. HDD 1GB SCSI타입
관리자로 접속한 뒤 fdisk -l 로 해당 하드디스크가 인식되는지, 파티션이 만들어져있는지를 확인한다.(파티션이 생성되어있지 않다면 fdisk /dev/sdb 로 파티션을 생성하자)</p>

<p>blkid(또는 blkid /dev/sdb3) 로 포맷, 파일시스템 생성여부를 살펴본다. UUID가 나오지 않는다면 이는 포맷이 되지 않은 것이므로 mkfs.xfs /dev/sdb3 또는 mkfs -t xfs /dev/sdb3 로 포맷을 진행한다.</p>

<p>/dev/sdb3 라는 장치를 공유할 것인데 먼저 vi /etc/exports 로 들어가자.
/임의의디렉토리	제공받을대상IP(옵션)
/Server/sdb3		192.168.52.0/255.255.255.0(rw,no_root_squash,sync)
을 추가한다.
:wq 로 저장 후 종료</p>

<p>/dev/sdb3의 마운트는 /Server에 할 것이며 nfs서비스로 제공 할 것은 /Server/sdb3이다.</p>

<p>mkdir -p /Server/sdb3 을 한 뒤에 /dev/sdb3를 /Server에 마운트하면 하위에 있던 sdb3 디렉토리는 사라지므로 먼저 mkdir /Server만 우선 해주도록 하자.</p>

<p>그리고 mount /dev/sdb3 /Sever로 마운트를 시킨다. 그 다음에 mkdir /Server/sdb3 로 디렉토리를 하나 더 생성하자.</p>

<p>/dev/sdb3 에 대해 자동마운트를 설정 할 것이다.
vi /etc/fstab 으로 들어간 뒤에 :se nu를 한다.
맨 밑에줄에 새로 추가를 할 것인데 장치명으로 추가 할 것이 아니라 UUID 로 추가할 것이다. 
vi 명령모드에서 ‘:줄수! 명령어’ 로 추가하면 된다. “:14! blkid /dev/sdb3” → 15번째 줄에 명령어 수행결과값이 들어감
UUID값	/Server		xfs		defaults	0 0
(또는 /etc/fstab 내용 설정 후 마운트를 mount -a 로 할 수도 있다. 이 때에는 마운트디렉토리가 존재해야한다.. reboot하면 자동으로 마운트디렉토리까지 만들어주기는 한다.)
systemctl restart nfs.service 로 서비스 제공준비 완료.
이미 켜놓은 상태였다면, /etc/exports 내용 수정시 exportfs -r 로 재반출적용 하면 된다.
(이렇게 exportfs 명령어가 따로 존재하는 이유는, nfs서비스 자체를 재시작을 하면 rpcbind에서 인식하는 nfs포트가 바뀌기 때문이다.)</p>

<p>이제 클라이언트입장.
rpm -qa | grep autofs로 패키지 존재여부 확인.
존재하지 않는다면 yum install autofs*로 설치하고 rpm -qa|grep autofs로 재확인</p>

<p>vi /etc/autofs.conf로 기본 설정파일로 들어가자. 
:se nu를 하고 45번째 줄로 가서 browse mode를 yes로 만든다.
:wq로 종료</p>

<p>vi /etc/auto.master 에서는 건드릴 것이 없다.
자동마운트 상위 디렉토리 /misc디렉토리를 쓰기 싫다면 지우고 다른 것을 작성하고 쓰면 된다.</p>

<p>vi /etc/auto.misc 로 마운트디렉토리 설정파일로 들어간다.
/misc하위에마운트받을디렉토리		-옵션		서버IP:디렉토리</p>

<p>혹시..
Client/sdb3		-rw		192.168.52.149:/Server/sdb3
이렇게 쓰면 /misc/Client/sdb3에 마운트가 될 까?
systemctl restart autofs.service를 한 뒤에 ls -l /misc를 보자.
→ 안된다. 이렇게 하면 아예 /misc 하위에 Client디렉토리는 보이지도 않는다. cd디렉토리만 보임</p>

<p>다시 vi /etc/auto.misc로 들어가서
Client_sdb3		-rw		192.168.52.149:/Server/sdb3
로 바꾼다.</p>

<p>그리고 systemctl restart autofs.service를 하면 /misc/Client_sdb3가 잘 생긴다.</p>

<p>touch /misc/Client_sdb3/helloserver 로 파일을 생성하고 ls -l /misc/Client_sdb3로 내가 만든 파일을 볼 수 있다.
이는 서버입장에서도 ls -l /Server/sdb3 명령어를 통하여 파일을 확인할 수 있다.</p>

<p>이제 reboot 하여 확인해보자.
→ 근데 nfs서비스도 꺼지고 autofs서비스도 꺼질텐데..
서버입장에서 먼저 제공을 해야 클라이언트에서 받을 수 있으므로 nfs를 먼저 키고 autofs를 켜야한다.</p>

<p>선생님이 개별 컴퓨터에 파일을 보낼 때 scp사용(ssh가 깔려있어서 사용 가능)
scp 내파일 상대방컴퓨터에접속할계정명@상대방IP:/목적지디렉토리
scp /etc/auto.misc root@192.168.52.51:/root
그리고 상대방 root계정의 비밀번호를 입력하면 전송 완료</p>

<p>reboot을 한 뒤에 
systemctl restart nfs.service로 서버입장에서 서비스를 재시작
systemctl restart autofs.service로 클라이언트 입장에서 서비스를 재시작</p>

<p>ls -l /misc 를 해보면 Client_sdb3가 남아 있는 것을 볼 수 있다.
이 때 mount | tail -1 을 하면 마운트는 안되어있는 것으로 보인다.</p>

<p>다시 ls -l /misc/Client_sdb3를 하면 helloserver파일이 남아있는 것을 볼 수 있는데 이러면 mount | tail -1로 마운트 된 것을 확인 할 수 있다.(이렇게 직접적으로 마운트포인트 디렉토리를 사용 할 때 자동마운트가 되는 것을 확인 할 수 있다)
역시나 서버입장에서 ls -l /Server/sdb3로 helloserver파일이 존재하는 것을 볼 수 있다.</p>

<p>재부팅을 한 뒤에 systemctl restart로 서비스를 재시작 해줘야 하는 것에 대하여.
서비스도 자동으로 켜지도록 하는 방법 → 서버에서의 nfs서비스 및 클라이언트에서의 autofs서비스에 대하여.</p>

<p>자동서비스
→ putty나 OpenSSH를 쓸 때 우리는 리눅스 머신에 대해 재부팅을 했음에도 해당 서비스를 시작시킨적이 없다. 그럼에도 해당 서비스를 계속 잘 쓸 수 있었다. 이는 sshd 서비스가 자동으로 켜지도록 설정되어있었기 때문이다.(물론 방화벽에 있어서도 항상 자동으로 포트개방이 되어있도록 설정도 되어있었을 것임)</p>

<p>→ 이는 firewall-cmd와 관련이 되어있는 것인지 아니면 슈퍼데몬과standalone과 관련이 있는것인자
⇒ firewall-cmd 는 해당 서비스에 대한 포트를 항상 열어둘 것인지에 대해 설정하는 부분으로서 서비스 자체의 자동시작과는 관련이 없다.</p>

<ul>
  <li>시스템이 부팅할 때 자동으로 설정한 서비스를 딱 한번 시작을 자동으로 해주는 역할</li>
  <li>자주 사용하는 서비스를 설정해두면 효율적으로 작업할 수 있음</li>
  <li>기본값으로 설정 되어있는 것 이외에 별도의 해당 서비스 적용시 본인이 직접 해야함</li>
</ul>

<p>서비스 자동시작 설정 명령어
CentOS 6.x	CentOS 7.x
chkconfig SERVICE(DAEMON) on	systemctl enable SERVICE(DAEMON)
chkconfig SERVICE(DAEMON) off	systemctl disable SERVICE(DAEMON)
chkconfig –list SERVICE(DAEMON)
→ 해당서비스가 자동으로 되어있는지 아닌지 나온다.	systemctl is-enabled SERVICE(DAEMON)</p>

<p>systemctl list-dependencies –before SERVICE(DAEMON)
→ 해당 서비스 이전에 실행되어? 있어야하는 의존성 서비스 목록 확인</p>

<p>systemctl list-dependencies –after SERVICE(DAEMON)
→ 해당 서비스가 실행되어야 활성화 될 수 있는 서비스 목록 출력
chkconfig –list
→ 전체 서비스목록 출력(자동인지 아닌지와 함께)	systemctl list-unit-files –type service
<img width="619" alt="스크린샷" src="https://user-images.githubusercontent.com/39452092/82836045-bee95300-9f00-11ea-9f9c-8ed0f5c1ef7a.png" /></p>

<p>chkconfig 는 7.x에서는 더이상 먹히지 않는다.
systemctl list-dependencies –before nfs.service 하면 rpcbind가 나올 것 같았고
systemctl list-dependencies –after rpcbind.service하면 nfs가 나올 것 같았는데 그렇진 않았다. 의존성은 조금 더 까다로운 문제인 것 같다.</p>

<p>Client머신에서 자동서비스로 nfs와 autofs를 추가하고 머신을 껐다 켜보자. 자동서비스로 추가가 잘 됐다면 해당 서비스는 재부팅후에 자동으로 켜져있을 것이며 그렇다면 서버에서 제공하는 nfs서비스와 클라이언트에서의 autofs가 연결이 알아서 잘 되어있을 것이다.</p>

<p>먼저 systemctl is-enabled nfs.service 와 systemctl is-enabled autofs.service를 확인해보자.
→ disabled라고 뜬다.</p>

<p>systemctl enable nfs.service와 systemctl enable autofs.service를 치자.
다음부터 자동실행될 수 있도록 바로가기를 만들었다는 메시지가 나온다.</p>

<p>활성화 시킨뒤에 systemctl is-enabled를 해보면 enabled가 뜨는 것을 확인 할 수 있다.</p>

<p>재부팅을 한 뒤에 systemctl restart를 하지 말고 nfs와 autofs 연결이 잘 되어있는지를 확인해보자
→ 왜 학원에서 재부팅 후에 ls -l /misc/client300 해보면 ‘cannot access /misc/client300/: No such file or directory가 뜨는것인가.. ls -l /misc를 하면 client300 디렉토리를 볼 수 있고 systemctl is-enabled 하면 둘다 활성화 되어있는 것을 볼 수 있는데.. 
⇒ 집에서는 잘 된다. 재부팅후에 확인해보니 알아서 연결되어있다.</p>

<p>⇒ firewall-cmd는 방화벽설정에 대해 자동으로 키는 것을 말하는 건가
그리고 이렇게 nfs enable하고 autofs enable순서로 활성화 시켜서 서비스가 켜지는 순서가 nfs → autofs인건지.. 만약 autofs가 먼저 작동하는 거였다면 오류가 발생하지 않았을까</p>

<p>서비스를 자동으로 실행시키는 것을 많이 해두면 부팅 속도가 느려질 수 있다.</p>

<p>nfs service와 autofs service 최종 실습
서버와 클라이언트를 모두 마지막으로 찍어둔 스냅샷으로 돌리고 진행(총 2대)</p>

<p>nfs server
HDD(SCSI, 1GB) partition은 5개 설정을 하되 사용하는 파티션은 자유롭게, 용량도 자유롭게
nfs service 설정할 때 아래를 참고
첫번째 호스트가 읽고 쓰기가 가능하며 nfs client의 root를 무시하지 않고 인정
두번째 네트워크 대역으로 읽고 쓰기가 가능하며 nfs client의 root를 무시
세번째 모든 네트워크가 읽기 전용으로 가능하며 nfs client의 root를 무시
auto mount <br />
—————————————————————————- <br />
nfs client
nfs server가 공유한 파일 시스템을 nfs service를 이용하여 설정한 뒤에 nfs server와 공유되는지 확인
nfs server가 공유한 파일 시스템을 autofs service를 이용하여 설정한 뒤에 nfs server와 공유되는지 확인
nfs server가 공유한 파일 시스템을 nfs service 또는 autofs service를 이용하여 설정한 뒤에 nfs server와 공유되는지 확인</p>

<p>(참고로
nfs server와 nfs client가 서로 rpc 통신상태 확인해보기
→ rpcbind통신여부 확인해야 하는 것이다. rpc연결여부 확인 명령어는 rpcinfo
autofs service로 설정 했으면 반드시 재부팅하여 nfs server와 공유되는지 확인
다 하신 분들은 시간이 되면 nfs client가 /etc/fstab 파일을 사용하여 작업도 해보도록 하겠습니다)</p>

<p>먼저 Server머신에 하드디스크를 추가한다.
터미널창을 열고 nfs서비스가 설치되어있는지 확인
rpm -qa|grep nfs 를 하면 nfs-utils 가 설치되어있는 것을 확인할 수 있다. 우리는 nfs4-acl-tools 패키지가 더 필요하다. yum install nfs4-acl-tools 로 패키지를 설치하자.(또는 yum install nfs*)
→ 혹시 설치가 안된다면 VMware 네트워크설정에서 VMnet0 이 브릿지로 되어있는지 보자. 안되어있다면 브릿지로 바꾼 뒤 브릿지할 네트워크에서 virtual box는 체크해제해야함
프로세스 종료는 kill -9 PID 로 하면 된다. -9는 시그널숫자로서 강제종료를 의미한다.</p>

<p>설치를 완료하고 rpm -qa|grep nfs로 확인하자.
→ 설치과정에서 rpcbind가 설치되어있지 않다면 자동으로 알아서 설치해준다.(yum덕분)</p>

<p>이제 하드디스크 장치에 대하여 설정을 하자.
fdisk -l 로 장치명 확인 뒤 fdisk /dev/sdb로 파티션 생성창 진입
5개의 파티션의 용량은 각각 100, 200, 300, 350, 50Mbyte로 잡자.
주파티션 3개에 Extended를 하나 만들고 논리파티션으로 2개를 잡아주면 된다.
w로 저장 후 종료한다.</p>

<p>이제 파일시스템을 만들자
mkfs.xfs /dev/sdb1 또는 mkfs -t xfs /dev/sdb1로 만들면 되고 확인은 blkid로 가능하다.</p>

<p>먼저 마운트할 마운트디렉토리를 만들 것이다.
/dev/sdb1 → /100
/dev/sdb2 → /200
/dev/sdb3 → /300</p>

<p>mkdir /100 /200 /300 모두 생성해주자.</p>

<p>자동마운트설정
vi /etc/fstab으로 들어가 아래의 내용을 추가하였다.
/dev/sdb1       `                         /100                    xfs     defaults        0 0
/dev/sdb2                                 /200                    xfs     defaults        0 0
/dev/sdb3                                 /300                    xfs     defaults        0 0
:wq로 저장 후 종료</p>

<p>mount -a 로 자동마운트를 시키자.</p>

<p>이제 vi /etc/exports 설정을 해주자.
#[share dir]            <a href="option">access allow host/network</a>
/100                    192.168.52.122(rw,no_root_squash,sync)
/200                    192.168.52.0/24(rw,sync)
/300                    *(sync)
의 내용을 추가하였다. 
읽기 전용이나 root_squash는 기본값이므로 작성해주지 않아도 된다. → 작성하면 오히려 오류나지 않던가?</p>

<p>systemctl restart nfs.service로 서비스 재시작(만약 이미 켜져있었다면 exportfs -r 쓴다.)
systemctl -l status nfs.service로 서비스 이상유무 판단
systemctl enable nfs.service로 서비스 자동시작 설정</p>

<p>방화벽설정까지 미리 해주자. 
방화벽 public에서 rpc-bind와 nfs 및 nfs3까지 해제해주었다.(영구적)</p>

<p>이제 서버에서의 설정은 모두 끝났다.</p>

<p>Client머신으로 들어가자.
서버의 /100 은 클라이언트에서는 /Client100 으로 마운트를 할 것이고 
서버의 /200과 /300은 클라이언트에서 /misc 하위에 /Client200 과 /Client300이라는 이름으로 자동마운트되도록 만들 것이다.</p>

<p>먼저 nfs패키지 설치여부 점검 → rpm -qa | grep nfs
설치되어있지 않으며 yum install nfs<em>로 설치(사실 nfs-utils 및 nfs4-acl-tools만 설치해줘도 된다.)
autofs패키지 설치여부 점검 → rpm -qa | grep autofs
설치되어있지 않으며 yum install autofs</em>로 설치</p>

<p>먼저 서버의 /100 디렉토리를 연결하기 위해 마운트포인트디렉토리를 만들자.
mkdir /Client100
mount -t nfs 192.168.52.149:/100 /Client100</p>

<p>df -h 명령어나 mount명령어로 마운트여부 확인 가능하다.</p>

<p>이제 서버의 /200과 /300 디렉토리를 연결하자.
먼저 vi /etc/autofs.conf로 들어가서 browse mode를 yes로 활성화한다.
:wq로 저장 후 종료</p>

<p>/etc/auto.master는 건드릴 필요가 없다.
바로 vi /etc/auto.misc로 들어간다.</p>

<p><img src="https://user-images.githubusercontent.com/39452092/82836092-d9233100-9f00-11ea-9933-ac91c03a37c7.png" alt="image" /> <br />
의 내용을 추가하였다. 이 때 /300 에 대해 서버는 ro 로서만 서비스를 제공하는데 클라이언트에서 rw로 쓰려고 한다면 어떻게 될까?</p>

<p>:wq로 저장 후 종료</p>

<p>systemctl restart autofs.service로 서비스 재시작하자.
ls -l /misc 했는데 디렉토리 안보임…</p>

<p>vi /etc/auto.misc 다시 들어갔는데.. 아차 맨 앞에 슬래시 쓰면 안된다.</p>

<p><img src="https://user-images.githubusercontent.com/39452092/82836100-df191200-9f00-11ea-93a0-8dea1b163d29.png" alt="image" />  <br />
로 수정 후 :wq로 저장 후 종료하였다.</p>

<p>systemctl restart autofs.service로 재시작
ls -l /misc하니까 디렉토리 잘 보인다.</p>

<p>현재로서는 마운트포인트(/misc/Client200 과 /misc/Client300)에 아무것도 안했어서 실질적으로는 마운트 연결이 안되어있는 상태이다. df -h 나 mount를 써도 마운트가 안되어있다.
따라서 ls -l /misc/Client200 과 ls -l /misc/Client300과 같이 간단히 해당 디렉토리를 쓰는 명령어를 사용만 해줘도 df -h 와 mount를 쓰면 마운트가 된 것을 볼 수 있다.</p>

<p>일단은 서비스를 자동재시작 설정해두자.
nfs타입으로만 마운트한 것은 어차피 재부팅 할 때마다 내가 일일히 mount -t nfs로 재마운트 해야한다.(nfs.service를 자동재시작 해놓는다고 해봤자 그건 서버입장에서 nfs서비스 제공을 자동으로 만들뿐 클라이언트입장에서 마운트까지 자동으로 해주는 것은 아니다.)
autofs만 자동재시작해놓자.
systemctl enable autofs.service입력.</p>

<p>파일을 만들어보자.
touch /Client100/helloserver1 /misc/Client200/helloserver2 /misc/Client300/helloserver3</p>

<p><img src="https://user-images.githubusercontent.com/39452092/82836107-e4765c80-9f00-11ea-97ad-e37ee28f4204.png" alt="image" />  <br />
서버의 /200 에는 접근이 거부되었다. Client머신이 root계정인데 서버에서는 root 접근을 막아놓았기 때문(익명계정으로 접근됨)
/300은 Read-only이므로 파일을 작성할 수가 없다.(서버의 /etc/exports에서도 ro로만 제공을 하며 클라이언트의 /etc/auto.misc 에서도 ro로만 받도록 해놓음)</p>

<p>따라서 현재는 /Client100 에 만든 파일만이 존재한다. ls -l /Client100으로 확인 가능하며 서버에서는 이를 ls -l /100으로 확인 가능하다.</p>

<p>root계정이 아닌 다른 계정으로 서버의 /200에 파일을 만들어보자
su - jinghong
잠시 내 계정으로 들어왔는데 여전히 df -h 해보면 연결은 유지되어있음을 볼 수 있다.</p>

<p><img src="https://user-images.githubusercontent.com/39452092/82836117-eb9d6a80-9f00-11ea-9ad4-7bdbed014262.png" alt="image" />  <br />
이미 root계정으로 연결해놓은거라 파일작성이 불가능한건가
→ 이러면 아예 처음부터 내 계정으로 접속해서 내 홈디렉토리 하위의 임의의 디렉토리등에 서버의 /200을 autofs형식으로 마운트시켜야하나??? 흠
⇒ 서버에서는 root계정에 대해 익명계정으로 처리되도록 root_squash가 활성화되어있다. 나는 이것에 대해 단순히 익명사용자계정으로는 파일을 쓸 수 없는 것인줄 알았다. 그래서 내 일반사용자계정으로는 파일을 쓸 수 있지 않을까 했던 것이다.
하지만 이 상태에서는 root로든 일반사용자계정으로든 파일을 쓸 수 없다. 그 이유는 서버에서 /200에 대해 허가권 설정을 rwxr-xr-x 로 해두었기 때문이다. 즉 root가 익명사용자로 변해 접근하는 것이든 일반사용자이든 other에 속하는데 w권한이 없으므로 파일을 쓸 수 없다.</p>

<p>일단 Client머신에서 재부팅해보자.
재부팅 후 df -h를 해보면 서버의 /100 /200 /300 이 다 마운트가 해제되어있다.
systemctl is-enabled autofs.service 해보면 enabled되어있으며 ls -l /misc 하면 Client200 과 Client300이 다 있는 것을 확인 할 수 있다. → 해당 디렉토리에 대해 어떤 작업을 하면 자동마운트가 될 것이다.
따라서 Client100 연결만 다시 해주자.
mount -t nfs 192.168.52.149:/100 /Client100 하면 연결이 된다.</p>

<p>근데 지금 root계정인 상태에서 touch /misc/Client200/helloserver2가 되나?
su - jinghong을 했음에도 안되고
sudo  touch /misc/Client200/helloserver2 도 안되는데..
sudo jinhong touch /misc/Client200/helloserver2 하면 jinhong 명령어를 못찾는다고 나온다.
아예 내 계정으로 들어가서 따로 또 연결을 하고 파일을 만들 수도 있을 것이다. → 서버의 /200 디렉토리 허가권에 있어서 other에 w가 없는한 불가능</p>

<p>→ 허가권 건드려보자. 클라이언트에서 접근하는 root계정에 대해 익명처리가 되므로 이는 서버머신입장에서는 자신의 디렉토리에 other권한으로 접근하려고 하는 것이다.
아예 o+w 권한을 줄 수도 있지만 sticky bit를 주자.
chmod o+t /200 을 하고 ls -ld /200 으로 확인 → rwxr-xr-t 로 됨
sticky bit 줬는데도 클라이언트에서 root계정으로 파일 생성이 안된다.(익명으로 처리되어서 그런건가. 익명계정은 아예 sticky bit 디렉토리에 파일 쓰기 불가능? jinhong계정이었다면 생성가능?)
아예 o+w를 주니까 클라이언트에서 root계정으로도 파일생성이 가능해졌다.(익명계정으로서 파일을 생성한 상태. 소유권은 nfsnobody라고 뜬다. 이 상태에서는 당연히 jinhong계정으로도 파일 생성이 가능 할 것이다.)
⇒ sticky bit를 준 상태 일 때 root로서 익명계정 접근 파일생성 불가능한 것은 보았다.
jinghong계정으로는 될 줄 알았는데 안된다.(su - jinhong 후 파일생성시도함)
왜 sticky bit가 안먹히는걸까.</p>

<p>현재 서버의 /200 디렉토리는 rw는 있지만 root_squash인 상태이다. 따라서 클라이언트에서 root계정으로 파일을 쓰려고 한다면 이는 익명계정으로 처리가 되며 /200허가권의 other에 들어가게 된다. 따라서 파일을 쓸 수 없게 됨. 이는 클라이언트에서 다른 사용자계정으로 파일을 쓰려 하는 경우에도 마찬가지가 된다. no_root_squash를 넣어주고 클라이언트에서 root 계정으로 파일을 쓰게 하던지(이경우에도 일반사용자계정은 other에 속하므로 파일 쓰기 불가), 아니면 허가권 부분을 건드려주면 된다. o+w해주면 root_squash가 되어있더라도 익명계정으로서 파일을 쓸 수 있으며 일반사용자도 파일을 쓸 수 있게 된다.</p>

<p>/300 에 대해서는 아예 read-only라서 파일을 쓸 수가 없다. 서버에서 이 디렉토리에 대해 rw로 해줘야한다.(/etc/exports설정필요), 클라이언트의 /etc/auto.misc에서도 서버의 해당 디렉토리를 -rw로서 끌어올 수 있도록 조정이 필요하다.
현재 /300디렉토리 또한 root_squash상태인데, 클라이언트에서 이 디렉토리에 대해 또 root로 접근하려는 경우 익명계정으로라도 파일을 쓰게 하려면 o+w를 해주던가 아니면 no_root_squash설정을 해주면 된다.
o+w를 해주면 일반사용자 계정으로도 파일은 쓸 수 있게 될 것이다.</p>

<p>→ 일반사용자로 서버의 nfs를 쓰려는경우(mount -t nfs를 통한 접근) 마운트포인트는 자신의 홈 디렉토리 하위에 둬야 할 것이다.. root가 쓰던 것을 그대로 쓸 수 있을까. 안될 것 같은데..
게다가 Autofs설정파일을 건드릴 수 있는가? 아니면 자신의 계정상에서 또 autofs를 설치해야하려나.. autofs 에 대한 설정파일인 /etc/auto.misc를 편집이나 가능할런지. 해당파일은 현재</p>

<p><img src="https://user-images.githubusercontent.com/39452092/82836120-f2c47880-9f00-11ea-9fe7-7c7800d45965.png" alt="image" />  <br />
라서 root만 편집가능하다.</p>

<p>rpc 통신상태 확인
명령어는 rpcinfo이며 
rpcinfo를 써도 되지만 rpcinfo -p 를 하면 내 rpcbind에 있어서 포트정보가 같이 나오며 rpcinfo -p 상대방IP를 하면 상대방 rpc통신을 볼 수 있다.</p>

<p>하지만 현재 서버에서는 rpcinfo -p로 자신의 정보를 볼 수는 있지만 클라이언트머신에는 rpc가 켜져있는게 아니라서 ‘rpcinfo -p 클라이언트머신IP’를 해도 상대방상태를 볼 수가 없다.
(클라이언트도 nfs를 쓸지는몰라도 rpc를 통해 외부의 접근을 제어하고있는상태가 아니다.)
이와 반대로 클라이언트에서는 rpcinfo -p 를 하면 현재 자신의 rpc 정보를 볼 수는 있는데, 보면은 portmapper만 활성화되어있다. 이 상태에서 ‘rpcinfo -p 서버IP’ 를 입력하면 서버의 rpc정보를 볼 수가 있다.
서버에서는 portmapper뿐만아니라 nfs까지 활성화가 되어있는 것을 볼 수 있는데, 클라이언트에서는 현재 portmapper만 활성화되어있는 것처럼 보인다. Portmapper만 켜져있다면 이는 외부의 접근을 기다리는중이며 외부에서 접근을 하는경우 nfs를 실질적으로 활성화시키면서 외부에서 nfs에 접근할 수 있도록 따로 포트번호를 부여받아서 nfs에 재할당해주는 것으로 보인다.
즉, 현재 서버에서는 nfs와 rpcbind(portmapper)가 다 가동중이며 rpcinfo를 통해 해당 정보들을 볼 수 있으나 클라이언트에서는 rpcbind를 쓰고 있는 것이 아니고 단지 서버의 nfs 서비스만을 이용중이므로 rpcbind가 비활성화(대기중)상태여서 rpcinfo를 볼 수 없는 것 같다. 물론 클라이언트에서는 자기 스스로 직접 rpcinfo를 볼 수는 있겠지만 서버에서는 rpcinfo -p 클라이언트IP 로 정보를 볼 수 없는건 서버가 직접 클라이언트의 rpc쪽에 연결되어있는 상태가 아니므로..</p>

<p>mount -t nfs로 클라이언트에서는 서버의 /100으로 접근을 시도했었다.
이는 클라이언트에서 재부팅시 마운트가 해제되어버린다.(설사 systemctl enable nfs.service를 한다고 해도! 이 서비스는 제공을 해주는 서비스일뿐이다. 클라이언트에서 항상켜져있게 해준다고 해도 자동마운트까지 해주지는 않는다.)
따라서 우리는 autofs를 설정해주었고 해당서비스도 언제나 실행모드로 만들어주었다.</p>

<p>클라이언트에서 Autofs를 쓰지 않고 /etc/fstab을 통하여 자동마운트를 시켜줄 수 있을까?
vi /etc/fstab 으로 들어가자.
디바이스명	마운트포인트	파일시스템타입	dump여부 시스템시작시점검여부
를 작성해주어야한다
디바이스장치명에 서버것을 쓸수나 있을런지.. 일단 해보자
192.168.52.149:/100 /Client100 xfs 0 0
이러고 재부팅 한 뒤에 연결 되어있는지 확인하면 된다</p>

<p>→ 안된다.. 집에서 vi /etc/fstab 으로 편집으로 들어가서 
192.168.0.15:/100	/Client100	xfs	0 0
추가하고 reboot했더니 emergency mode로 부팅이 된다.</p>

<p>root로그인 한 후에 df -h 해보면 자동마운트 되어있지 않음을 볼 수 있었다.
원래대로 복구하자.</p>

<p>vi /etc/fstab에서 이전 내용 제거 후 reboot
→ 정상부팅 됨. 완료</p>
:ET